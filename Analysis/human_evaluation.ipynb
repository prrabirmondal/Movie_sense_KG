{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Movie Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the aspect of a movie\n",
    "# display the nodes and ask for node-edge relation authenticity \n",
    "# display question from the aspects and ask for the meaningfulness and complexity level of the question.\n",
    "# save the authenticity score, question meaningfulness and complexity score with the ground truth value\n",
    "\n",
    "\n",
    "# process for each category:\n",
    "# -------------------------------\n",
    "# 1. sort the movie files from the dataset folder aspectwise\n",
    "# 2. preprocess the files based on the criteria we followed \n",
    "# 3. chose 30 percent of the non empty files with atleast 10 node-edge pairs and 10 questions using random sampling.\n",
    "# 4. choose on file with aspect, fetch its aspect information from the wikipedia page\n",
    "# 5. display its aspect content and then one by one present the node-edge pairs for authenticity test\n",
    "# 6. for each pair, receive the response over five scale and save the responses in a dictionary key will be the scale value and the value will be the count of the scale.\n",
    "# 7. after the nodes evaluation of the movie, display the 30 percent question sampled randomly.\n",
    "# 8. display the question, get the response of complexity test and save the match-mismatch score hopwise,questionwise. save the response in a dictionary with the proper keys and values.\n",
    "# 9. save the dictionary and file names already checked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def save_aspect(aspect, content, movie_name, yor, save_file_path = \"/mnt/Data/prabirmondal/prabir/python_program/movie_sense/SRI_KG/Movie_sense_KG/Movie_sense_KG/Analysis/movie.txt\"):\n",
    "    # print(f\"from save aspect, {aspect}\")\n",
    "    introduction = f\"Movie_name = {movie_name}, Year or release = {yor}.\\n HERE IS THE DETAILS OF MOVIE'S {aspect.upper()}: \\n\"\n",
    "    underline = \"-----------------------------------------------------\\n\"\n",
    "    para = introduction + underline + content\n",
    "    # print(para)\n",
    "    # input()\n",
    "    # print(para)\n",
    "    # Open the file in write mode and save the paragraph\n",
    "    with open(save_file_path, 'w') as file:\n",
    "        file.write(para)\n",
    "    \n",
    "    # print(f\"Paragraph saved to {save_file_path} for {aspect}.\")\n",
    "\n",
    "\n",
    "def scrape(wiki_link):\n",
    "    Aspects = [\n",
    "        'Plot',\n",
    "        'Cast',\n",
    "        'Production',\n",
    "        'Music',\n",
    "        'Soundtrack',\n",
    "        'Themes',\n",
    "        'Accolades',\n",
    "        ]\n",
    "\n",
    "    Aspects = [aspect.lower() for aspect in Aspects]\n",
    "    \n",
    "    # Send a request to the Wikipedia page\n",
    "    response = requests.get(wiki_link)\n",
    "    \n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    scraped_dict = {}\n",
    "    \n",
    "    #-------------------Summary scraping--------------------------\n",
    "    # Find the first paragraph after the title, which is usually the summary\n",
    "    summary_paragraphs = []\n",
    "    # Wikipedia's summary paragraphs are inside <p> tags but before any <h2> tag\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        \n",
    "        # Ensure the paragraph has text and is not empty\n",
    "        if paragraph.get_text().strip():\n",
    "            summary_paragraphs.append(paragraph.get_text().strip())\n",
    "        \n",
    "        # Stop once we hit the first section heading (e.g. 'Plot' or 'Contents')\n",
    "        if paragraph.find_next_sibling(['h2', 'h3']):\n",
    "            break\n",
    "    \n",
    "    scraped_dict[\"summary\"] =  ' '.join(summary_paragraphs)\n",
    "    \n",
    "    \n",
    "    # -------------------Other Aspect Scraping---------------------\n",
    "    Headings = soup.find_all('div', class_ = 'mw-heading mw-heading2')\n",
    "    for heading in Headings:\n",
    "        try:\n",
    "            aspect, _ = (heading.text).split('[')\n",
    "        except:\n",
    "            aspect = heading.text\n",
    "            \n",
    "        if aspect.lower() in Aspects:\n",
    "            next_siblings = heading.find_next_siblings()\n",
    "            text = ''\n",
    "            for next_sibling in next_siblings:\n",
    "                next_sibling_name = next_sibling.name\n",
    "                # sub_sibling = ''\n",
    "                # print(f\"............next_sibling_name = {next_sibling_name}\")\n",
    "                if (next_sibling_name =='style'):\n",
    "                    continue\n",
    "                \n",
    "                elif (next_sibling_name == 'div'):\n",
    "                    clss = next_sibling.get('class')\n",
    "                    \n",
    "                    if ('mw-heading2' in clss):  # break because heading ended\n",
    "                        break\n",
    "                text += \" \"+ next_sibling.text\n",
    "            scraped_dict[aspect] = text \n",
    "    return scraped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_node_df(df):\n",
    "    # Define a function to check if the edge contains both node names and at least 5 words\n",
    "    def is_valid_edge(row):\n",
    "        edge_words = row['edge'].split()\n",
    "        return (\n",
    "            len(edge_words) >= 5 and \n",
    "            row['node_1'] in row['edge'] and \n",
    "            row['node_2'] in row['edge']\n",
    "        )\n",
    "    \n",
    "    # Filter the dataframe using the validation function\n",
    "    filtered_df = df[df.apply(is_valid_edge, axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def randomPick_question(df):\n",
    "    # Create an empty dataframe\n",
    "    random_row = pd.DataFrame()\n",
    "    \n",
    "    # Create an empty dataframe with predefined columns\n",
    "    random_row = pd.DataFrame(columns=['Column1', 'Column2', 'Column3'])\n",
    "    \n",
    "    total_question = len(df)\n",
    "    if total_question >= 10:\n",
    "        sample_30_count = int(0.3*total_question)\n",
    "        # Randomly select a single row\n",
    "        random_row = df.sample(n=sample_30_count, random_state=seed_value)\n",
    "        # random_rows = df.sample(n=n, random_state=seed_value)\n",
    "    \n",
    "    return random_row\n",
    "\n",
    "def read_csv(file_path:str):\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except:\n",
    "            df = pd.DataFrame()\n",
    "    else:\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "    return df\n",
    "    \n",
    "\n",
    "def fetch_questions(category, node_file_name, hop):\n",
    "    questionFile_root = \"/mnt/Data/prabirmondal/prabir/python_program/movie_sense/SRI_KG/Movie_sense_KG/Movie_sense_KG/Dataset/Questions_Answers/Hollywood\"\n",
    "    \n",
    "    hop_yes_no_path = os.path.join(questionFile_root, category, hop, \"yes_no\", node_file_name)\n",
    "    hop_MCQ_S_path = os.path.join(questionFile_root, category, hop, \"MCQ_single_correct\", node_file_name)\n",
    "    hop_MCQ_M_path = os.path.join(questionFile_root, category, hop, \"MCQ_Multiple_correct\", node_file_name)\n",
    "    \n",
    "    \n",
    "    hop_yes_no_questions = randomPick_question(read_csv(hop_yes_no_path))\n",
    "    hop_MCQ_S_questions = randomPick_question(read_csv(hop_MCQ_S_path))\n",
    "    hop_MCQ_M_questions = randomPick_question(read_csv(hop_MCQ_M_path))\n",
    "    \n",
    "    # print(\"from fetch_questions\")\n",
    "    # print(\"-----------------------------\")\n",
    "    # print(len(hop_yes_no_questions))\n",
    "    # print(len(hop_MCQ_S_questions))\n",
    "    # print(len(hop_MCQ_M_questions))\n",
    "    \n",
    "    return hop_yes_no_questions, hop_MCQ_S_questions, hop_MCQ_M_questions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"YoR\", \"movie_name\", \"imdb_rating\", \"wiki_link\", \"popular\"]\n",
    "movie_links = pd.read_excel(\"/mnt/Data/prabirmondal/prabir/python_program/movie_sense/SRI_KG/Movie_sense_KG/Movie_sense_KG/aspect_wise_knowledge_graph/workspace/Movie_list.xlsx\", sheet_name = \"hollywood\", engine='openpyxl')\n",
    "movie_links.columns = columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gods of Egypt\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n",
      "Fantasy Island\n",
      "Baywatch\n",
      "Hot Pursuit\n",
      "The Oogieloves in the Big Balloon Adventure\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n",
      "In the Name of the King\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n",
      "All About Steve\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n",
      "Fifty Shades Freed\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n",
      "The Human Centipede 3 (Final Sequence)\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n",
      "BloodRayne\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n",
      "The Fog\n",
      "\n",
      " Please check the content from the movie.txt file and then response the questions here\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nodeFile_root = \"/mnt/Data/prabirmondal/prabir/python_program/movie_sense/SRI_KG/Movie_sense_KG/Movie_sense_KG/Dataset/Nodes_Edges/Hollywood\"\n",
    "questionFile_root = \"\"\n",
    "\n",
    "Categories = os.listdir(nodeFile_root)\n",
    "\n",
    "\n",
    "aspect_list = ['Accolades',\n",
    "'Cast',\n",
    "'Guardians of the Galaxy Vol',\n",
    "'Music',\n",
    "'Plot',\n",
    "'Production',\n",
    "'Soundtrack',\n",
    "'Themes',\n",
    "'summary']\n",
    "\n",
    "for category in Categories:\n",
    "    for aspect in aspect_list:\n",
    "        # 8. add aspect in both dataframe of node and question\n",
    "        \n",
    "        node_file_names = os.listdir(os.path.join(nodeFile_root, category))\n",
    "        node_file_names = [node_file_name for node_file_name in node_file_names if node_file_name.split(\".\")[0].split(\"_\")[-1] == aspect]\n",
    "        for node_file_name in node_file_names:\n",
    "            # 9. check if the file already covered then continue otherwise procede\n",
    "            \n",
    "            # get the context from wikipedia\n",
    "            movie_name, yor = node_file_name.split(\"_\")[0], node_file_name.split(\"_\")[1]\n",
    "            print(movie_name)\n",
    "            wiki_link = movie_links[movie_links.movie_name == movie_name][\"wiki_link\"].iloc[0]\n",
    "\n",
    "            scraped_dict = scrape(wiki_link)\n",
    "            \n",
    "            try:\n",
    "                node_file = pd.read_csv(os.path.join(nodeFile_root, category, node_file_name))\n",
    "\n",
    "                sample_30_count = int(0.3*len(node_file))\n",
    "                node_file = node_file.sample(n=sample_30_count, random_state=seed_value)\n",
    "                \n",
    "            except:\n",
    "                \n",
    "                continue\n",
    "                \n",
    "            if len(node_file)>=3:\n",
    "                \n",
    "                h1q1, h1q2, h1q3 = fetch_questions(category, node_file_name, \"Simple_hop1\")\n",
    "                h2q1, h2q2, h2q3 = fetch_questions(category, node_file_name, \"Moderate_hop2\")\n",
    "                h3q1, h3q2, h3q3 = fetch_questions(category, node_file_name, \"Complex_hop3\")\n",
    "                \n",
    "                try:\n",
    "                    #save the scraped aspect of the considered movie                \n",
    "                    save_aspect(aspect, scraped_dict[aspect], movie_name, yor)\n",
    "                    print(\"\\n Please check the content from the movie.txt file and then response the questions here\")\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")                    \n",
    "                    input()\n",
    "                    \n",
    "                    # 1. Evaluate nodes (use terminal clear in the eval method)\n",
    "                    eval_node_dict = eval_nodes(node_file) #returns only the counts\n",
    "                    \n",
    "                    # 2. update the node evaluation dataframe of this aspect\n",
    "                    \n",
    "                    # 3. Evaluate Questions\n",
    "                    eval_question_h1_dict = eval_question(h1q1, h1q2, h1q3)\n",
    "                    eval_question_h2_dict = eval_question(h2q1, h2q2, h2q3)\n",
    "                    eval_question_h3_dict = eval_question(h3q1, h3q2, h3q3)\n",
    "                    \n",
    "                    # 4. merge all the question evaluation\n",
    "                    \n",
    "                    # 5. update the question evaluation dataframe of this aspect\n",
    "                    \n",
    "                    # 6. save the movie file name so that recompute is omit\n",
    "                    \n",
    "                    # 7. check the movie count for taking decision for further process\n",
    "                    \n",
    "                    \n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a request to the Wikipedia page\n",
    "response = requests.get(\"https://en.wikipedia.org/wiki/Fantasy_Island_(film)\")\n",
    "\n",
    "# Parse the page content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "scraped_dict = {}\n",
    "    \n",
    "#-------------------Summary scraping--------------------------\n",
    "# Find the first paragraph after the title, which is usually the summary\n",
    "summary_paragraphs = []\n",
    "# Wikipedia's summary paragraphs are inside <p> tags but before any <h2> tag\n",
    "for paragraph in soup.find_all('p'):\n",
    "    \n",
    "    # Ensure the paragraph has text and is not empty\n",
    "    if paragraph.get_text().strip():\n",
    "        summary_paragraphs.append(paragraph.get_text().strip())\n",
    "    \n",
    "    # Stop once we hit the first section heading (e.g. 'Plot' or 'Contents')\n",
    "    if paragraph.find_next_sibling(['h2', 'h3']):\n",
    "        break\n",
    "\n",
    "scraped_dict[\"summary\"] =  ' '.join(summary_paragraphs)\n",
    "scraped_dict\n",
    "\n",
    "# # -------------------Other Aspect Scraping---------------------\n",
    "# Headings = soup.find_all('div', class_ = 'mw-heading mw-heading2')\n",
    "# for heading in Headings:\n",
    "#     try:\n",
    "#         aspect, _ = (heading.text).split('[')\n",
    "#     except:\n",
    "#         aspect = heading.text\n",
    "        \n",
    "#     if aspect.lower() in Aspects:\n",
    "#         next_siblings = heading.find_next_siblings()\n",
    "#         text = ''\n",
    "#         for next_sibling in next_siblings:\n",
    "#             next_sibling_name = next_sibling.name\n",
    "#             # sub_sibling = ''\n",
    "#             # print(f\"............next_sibling_name = {next_sibling_name}\")\n",
    "#             if (next_sibling_name =='style'):\n",
    "#                 continue\n",
    "            \n",
    "#             elif (next_sibling_name == 'div'):\n",
    "#                 clss = next_sibling.get('class')\n",
    "                \n",
    "#                 if ('mw-heading2' in clss):  # break because heading ended\n",
    "#                     break\n",
    "#             text += \" \"+ next_sibling.text\n",
    "#         scraped_dict[aspect] = text \n",
    "# return scraped_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Sample data for nouns\n",
    "nouns = [\"tree\", \"car\", \"river\", \"mountain\", \"city\", \"book\", \"ocean\", \"computer\", \"bird\", \"house\"]\n",
    "\n",
    "# Function to generate a sentence for the edge\n",
    "def generate_edge(node1, node2):\n",
    "    # torch.manual_seed(42)\n",
    "    include_nodes = random.choice([True, False])  # Randomly decide whether to include nodes\n",
    "    if include_nodes:\n",
    "        # Include node1 and node2 in the edge sentence\n",
    "        return f\"{node1} is connected to {node2} in some way.\"\n",
    "    else:\n",
    "        # Generate a random sentence without mentioning node1 and node2\n",
    "        sentences = [\n",
    "            \"There is a strong relationship here.\",\n",
    "            \"The connection is abstract but meaningful.\",\n",
    "            \"This is a symbolic link.\",\n",
    "            \"A significant association exists.\",\n",
    "            \"A conceptual bond is present.\"\n",
    "        ]\n",
    "        return random.choice(sentences)\n",
    "\n",
    "# Generate 10 rows for the DataFrame\n",
    "data = []\n",
    "for _ in range(10):\n",
    "    node1, node2 = random.sample(nouns, 2)  # Randomly select two different nouns\n",
    "    edge = generate_edge(node1, node2)     # Generate the edge sentence\n",
    "    data.append({\"node_1\": node1, \"node_2\": node2, \"edge\": edge})\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B    C\n",
      "0  1  10  100\n",
      "1  2  20  200\n",
      "2  3  30  300\n",
      "3  4  40  400\n",
      "4  5  50  500\n",
      "Randomly selected rows:\n",
      "   A   B    C\n",
      "1  2  20  200\n",
      "4  5  50  500\n",
      "2  3  30  300\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def rndm(df):\n",
    "    # Randomly choose rows\n",
    "    n = 3  # Number of rows to select\n",
    "    df = df.sample(n=n, random_state=seed_value)\n",
    "    return df\n",
    "    \n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': [100, 200, 300, 400, 500]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# Set the seed value for reproducibility\n",
    "seed_value = 42\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "\n",
    "df = rndm(df)\n",
    "\n",
    "print(\"Randomly selected rows:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_file = pd.read_csv(\"/mnt/Data/prabirmondal/prabir/python_program/movie_sense/SRI_KG/Movie_sense_KG/Movie_sense_KG/Dataset/Nodes_Edges/Hollywood/Least_Popular/All About Steve_2009_Plot.csv\")\n",
    "\n",
    "# print(pd_file.head())\n",
    "\n",
    "print(len(pd_file))\n",
    "pd_file.columns\n",
    "\n",
    "pd_file = preprocess_node_df(pd_file)\n",
    "pd_file.head()\n",
    "\n",
    "# len(pd_file)\n",
    "# pd_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_file = pd_file.sample(n=4)\n",
    "pd_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, row in pd_file.iterrows():\n",
    "    node1 = row[\"node_1\"]\n",
    "    node2 = row[\"node_2\"]\n",
    "    edge = row[\"edge\"]\n",
    "    print(row[\"firstname\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
