{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrap_data(wiki_link, save_file_path = \"/Prabir/knowledge_graph/data_input/cureus/movie.txt\"):\n",
    "    wikipedia_movie_link = wiki_link\n",
    "    \n",
    "    page_to_scrape = requests.get(wikipedia_movie_link)\n",
    "    soup = BeautifulSoup(page_to_scrape.text, \"html.parser\")\n",
    "    \n",
    "    para = ''\n",
    "    for paragraph in soup.select('p'):\n",
    "        p = paragraph.getText()\n",
    "        para += p\n",
    "\n",
    "\n",
    "     # Open the file in write mode and save the paragraph\n",
    "    with open(save_file_path, 'w') as file:\n",
    "        file.write(para)\n",
    "    \n",
    "    print(f\"Paragraph saved to {save_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from helpers.df_helpers import documents2Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input data directory\n",
    "data_dir = \"cureus\"\n",
    "inputdirectory = Path(f\"./data_input/{data_dir}\")\n",
    "## This is where the output csv files will be written\n",
    "out_dir = data_dir\n",
    "outputdirectory = Path(f\"./data_output/{out_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_document(inputdirectory):\n",
    "#     loader = DirectoryLoader(inputdirectory, show_progress=True)\n",
    "#     documents = loader.load()\n",
    "    \n",
    "#     splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1500,\n",
    "#         chunk_overlap=150,\n",
    "#         length_function=len,\n",
    "#         is_separator_regex=False,\n",
    "#     )\n",
    "    \n",
    "#     pages = splitter.split_documents(documents)\n",
    "#     # print(\"Number of chunks = \", len(pages))\n",
    "#     # print(pages[3].page_content)\n",
    "\n",
    "#     # Create dataframe of chunks\n",
    "#     df = documents2Dataframe(pages)\n",
    "#     print(\"from load document\")\n",
    "#     print(df.head())\n",
    "#     # df.head()\n",
    "    \n",
    "    # return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install -U langchain-community\n",
    "# !pip install unstructured\n",
    "# !sudo apt-get install libmagic1\n",
    "# !pip install yachalk\n",
    "#ollama serve\n",
    "#ollama run zephyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function uses the helpers/prompt function to extract concepts from text\n",
    "from helpers.df_helpers import df2Graph\n",
    "from helpers.df_helpers import graph2Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def node_generation(df):\n",
    "#     ## To regenerate the graph with LLM, set this to True\n",
    "#     regenerate = True\n",
    "    \n",
    "#     if regenerate:\n",
    "#         concepts_list = df2Graph(df, model='zephyr:latest')\n",
    "#         dfg1 = graph2Df(concepts_list)\n",
    "#         if not os.path.exists(outputdirectory):\n",
    "#             os.makedirs(outputdirectory)\n",
    "        \n",
    "#         dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n",
    "#         df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\n",
    "#     else:\n",
    "#         dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n",
    "    \n",
    "#     dfg1.replace(\"\", np.nan, inplace=True)\n",
    "#     dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "#     dfg1['count'] = 4 \n",
    "#     ## Increasing the weight of the relation to 4. \n",
    "#     ## We will assign the weight of 1 when later the contextual proximity will be calculated. \n",
    "\n",
    "#     print(\"from node generation\")\n",
    "#     print(dfg1.shape)\n",
    "#     print(dfg1.head())\n",
    "\n",
    "#     return dfg1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strat from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"YoR\", \"movie_name\", \"imdb_rating\", \"wiki_link\", \"popular\"]\n",
    "movie_links = pd.read_excel(\"Movie_list.xlsx\", sheet_name = \"hollywood\")\n",
    "movie_links.columns = columns\n",
    "# movie_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_movie_links = movie_links[movie_links.popular == \"popular\"]\n",
    "least_popular_movie_links = movie_links[movie_links.popular == \"Least popular\"]\n",
    "# least_popular_movie_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_output_folder = \"/Prabir/knowledge_graph/hollywood/\"\n",
    "movie_categories = [least_popular_movie_links, popular_movie_links]\n",
    "\n",
    "for movie_category in movie_categories:\n",
    "    for index, row in movie_category.iterrows():\n",
    "        try:\n",
    "            movie_name = row[\"movie_name\"]\n",
    "            YoR = row[\"YoR\"]\n",
    "            wiki_link = row[\"wiki_link\"]\n",
    "            popular = row[\"popular\"]\n",
    "\n",
    "            # scrape data from the given link and store the data in the data_input directory\n",
    "            scrap_data(wiki_link)\n",
    "\n",
    "            #load document in dataframe chunk\n",
    "            loader = DirectoryLoader(inputdirectory, show_progress=True)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=1500,\n",
    "                chunk_overlap=150,\n",
    "                length_function=len,\n",
    "                is_separator_regex=False,\n",
    "            )            \n",
    "            pages = splitter.split_documents(documents)\n",
    "\n",
    "            \n",
    "            # Create dataframe of chunks\n",
    "            df = documents2Dataframe(pages)\n",
    "\n",
    "            #node generation task\n",
    "            ## To regenerate the graph with LLM, set this to True\n",
    "            regenerate = True\n",
    "            \n",
    "            if regenerate:\n",
    "                concepts_list = df2Graph(df, model='zephyr:latest')\n",
    "                dfg1 = graph2Df(concepts_list)\n",
    "                if not os.path.exists(outputdirectory):\n",
    "                    os.makedirs(outputdirectory)\n",
    "                \n",
    "                dfg1.to_csv(outputdirectory/\"graph.csv\", sep=\"|\", index=False)\n",
    "                df.to_csv(outputdirectory/\"chunks.csv\", sep=\"|\", index=False)\n",
    "            else:\n",
    "                dfg1 = pd.read_csv(outputdirectory/\"graph.csv\", sep=\"|\")\n",
    "            \n",
    "            dfg1.replace(\"\", np.nan, inplace=True)\n",
    "            dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "            dfg1['count'] = 4 \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            #save the nodes dataframe in csv_file\n",
    "            save_folder_name = popular\n",
    "            save_file_name = movie_name + \"_\" + str(YoR) + \".csv\"\n",
    "            save_path = os.path.join(root_output_folder, save_folder_name, save_file_name)      \n",
    "            dfg1.to_csv(save_path, index=False)\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
